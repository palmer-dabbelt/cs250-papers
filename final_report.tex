\documentclass[twocolumn]{article}
\usepackage{fullpage}
\usepackage{hyperref}

\title{CS250: An Elliptic Curve Cryptography Engine}
\author{Palmer Dabbelt and Kevin Linger}
\date{December 18, 2013}

\begin{document}
\maketitle

For our CS250 project, we implemented an elliptic curve cryptagraphy
engine that attaches via the Rocket coprocessor interface.  Our
accelerator was targeted at supporting the elliptic curve digital
signature algorithm (ECDSA), a NIST standard widely used on the
Internet for digital signatures.  We built off of previous work by
doing a more involved design-space exploration, focusing on
hardware/software co-tuning. We were succesful in implementing the
primary operation needed in ECDSA which is an elliptic curve point 
multiplication. A simulated version of the operation works with the
Rocket Core, but the full toolchain did not run. With some algorithmic
improvements to our accelerator, we can get tremendous improvements
over a software implementation.

\section{The Swarm}

FIXME: If we need to fill space I can stick in some blurb about
swarmos here
(I'm sure we will have more than plenty of space to fill, so go ahead)

\section{Elliptic Curve Cryptography}

Digital signature algorithms\cite{fips-186-3} are a widely used class
of algorithms that are central to the functioning of the modern
Internet.  DSA\cite{us-dsa} (the original digital signature algorithm
as standardized by NIST) is a specialization of the ElGamal signature
scheme\cite{elgamal-sig} that uses discrete logarithms over finite
integer fields.  Finite integer fields are particularly easy to
compute with electronics, so this gives DSA the advantage of being an
efficient signature algorithm.

The problem is that the discrete logarithm problem is not particularly
strong over finite integer fields (specifically, there are known
sub-exponential algorithms\cite{adleman-subexp}).  As shown in
Table~\ref{key-sizes}, DSA does not scale well to higher security
levels.  This is important because code breaking algorithms are
getting more efficient, leading to the need for higher security
levels, therefore imposing higher computational costs on every node in
the system. 

\begin{table}[h]
  \begin{center}
    \begin{tabular}{cccc}
      Security Level & DSA & ECDSA & Ratio \\
      \hline
      80 & 1024 & 160 & 3:1 \\
      112 & 2048 & 224 & 6:1 \\
      128 & 3072 & 256 & 10:1 \\
      192 & 7680 & 384 & 32:1 \\
      256 & 15360 & 521 & 64:1 \\
    \end{tabular}
  \end{center}

  \caption{Comparison of DSA and ECDSA\cite{nsa-case_for_ecc}
    \label{key-sizes}}
\end{table}

The commonly accepted solution to this problem is to change the field
from integers to elliptic curves, which (as shown in
Table~\ref{key-sizes}) scale much better to higher security levels.
While elliptic curves are significantly more difficult to build
electronics for, this will eventually be offset by sheer key size
alone.  The currently accepted security level is 128 bits, at which
ECDSA already has a 10:1 advantage in key size.  Shortening the key 
size becomes even more important, as we approach a world in which 
ubiquitous embedded systems must be able to securely transmit and 
receive data.

In addition to providing for faster cryptographic operations, this
shortened key size plays an important role specific to the swarm
related to key generation.  It is already known\cite{halderman-shared}
that we can't find sufficient entropy to generate RSA-style keys for
the machines we have, much less for a network the size of that
enviosned by the swarm project.  The main problem with generating RSA
keys is finding enough entropy to search for large prime numbers, a
problem that goes away with ECDSA because the numbers are smaller and
there's no necessity for them to be prime (there is an acceptability
criteria, but the chances of a random number not meeting it are
exceedingly low).

\section{Introduction to Elliptic Curve Arithmetic}

The computate-intensive operation in ECDSA is an elliptic curve (EC) point 
multiplication. An EC multiply is a series of EC doubles and additions, which
are much different than the respective integer operations. These operations
are defined geometrically; e.g. to double a point on an EC, a tangent line at that
point is drawn which will intersect the curve at exactly one other point. The result 
of the double operation is that intersection point reflected over the X-axis.
The add and double operations are defined algebraically by the following equations:

- Add equation

- Double equation

In hardware, these operations condense down to control logic operating on additions, multiplications, 
and divisions. Each of these operations also has a modular operation at the end, where the modulus is a 
large prime number that represents the order of the EC. This makes multiplication and division
much more difficult. A division is now a modular inverse followed by a modular multiplication. 

The discrete logarithm problem of ECDSA comes from being able to share a generator point
G of an elliptic curve, and the result point R of a point multiplication G*d = R 
(where d is an integer). Given only R and G, it is not easy to solve for d. To find d, an
outsider with have to add G to itself until the result matched the output R. The number
of additions grows exponentially with the key size. The multiplier d is the private key, 
and verification of the point multiplication can now be done in logarithmic time because
the number of point doubling operations is known. 

The EC point multiplication is essentially a hierarchy of control logic that has the integer 
aritmetic at the base. This was the motivation of the hardware/software cotuning: at what level
of the hierarchy should the hardware/software division be made?

\section{Implementing ECDSA}

Unlike finite integer fields, elliptic curves do not map directly to
the hardware present in current microprocessors\cite{kss-ecdsa}. This
suggests that a hardware accelerator designed to compute over elliptic
curves should lead to a significantly more efficient implementation
than in software alone.  ECDSA requires more computation per bit than
DSA, so a hardware accelerator is necessary to fully take advantage of
the smaller key size.

Previous projects\cite{nnll-ecdsa_hw} have shown that ECC hardware can
be implemented efficiently, but were limited in the amount of
design-space exploration that was attempted.  \cite{mmm-hw_ecc}
explored the tradeoffs of different algorithms for computation, but
did not explore the design space of any particular algorithm, which
seems to be the extent of the design space exploration performed.  In
addition, the current state of the art appears to consist solely of
FPGA implementations.  Unfortunately these studies don't present power
numbers, which is problematic because energy is the main limiting
factor for swarm applications.

FIXME: Here's where we should describe the computational patterns of
ECDSA, use those flow charts.

\section{Software Test Bench}

The primary goal of this project was to explore the tradeoffs involved
in adding different amounts of hardware support for ECDSA to a
pre-existing chip.  In order to achieve this, we needed to have the
ability to generate a myriad of different software configurations that
coorespond to each of the accelerator instances we generated.

The current industry standard cryptographic software, OpenSSL, is very
difficult to hack on.  Instead of messing around with OpenSSL, we
decided to instead write our own implementation of ECDSA with the
explicit goal of being paramaterizable.  We could then generate an
instance of this software that cooresponds with every accelerator
configuration that made sense and fully test our hardware.

Unfortunately, this software implementation proved to be the weakest
point of this entire project.  As shown in Figure~\ref{results}, our
code is hundreds of times slower than OpenSSL -- this is after a
significant amount of tuning, our first implementations were hundreds
of thousands of times slower than OpenSSL!  This trouble can be
attributed to two shortcommings: algorithms and tuning.

OpenSSL uses a number of significantly more complicated cryptographic
operations that our code does.  Specifically, it uses a projective
point representation that allows for the conversion of a large number
of modular inversions to a single inversion at the end of the multiplication. 
Implementing these algorithmic improvements should show significant speedups for
both our software and hardware, so it's not entirely a bad thing.  We
estimate about a $20\times$ gain can be had from algorithmic
improvements alone!

In addition to using better algorithms, OpenSSL is implemented better
than our software was.  Specifically, OpenSSL contains hand-tuned
assembly for a number of critical paths that compilers appear to
generate bad code for (specifically thinking of carry chains for
big integer operations).  Part of the problem that our implementation
had was probably related to the fact that it attempts to be
paramaterizable, which almost certainly defeats a lot of compiler
hueristics (I'd love a way to tell a compiler to generate 4 different
optimized versions of a function, sweeping arcoss the 4 different
values that one particularly interesting input can have).

Despite these shortcomings, we don't believe that the software troubles
completely invalidate the results from this project.  We spent about
the same amount of time on both the software and hardware. 
More importantly, we have numbers comparing
against the full-strength OpenSSL code running on x86 (a platform that
OpenSSL is highly optimized for\cite{kasper-openssl_ecc}).  These
numbers are probably the most reasonable to compare our ASIC
implementation against, as they're the most realistic picture of the
current state of the art.

\section{ASIC Implementation}

Our ASIC implementation of ECDSA performs all elliptic field arithmetic,
including point multiplication, addition, doubling, as well as modular
multiplication and modular inversion. Our hardware supports key sizes
up to 256 bits, as that is the suggested security level.

We created instructions to expose each of the operations listed above. Operands
are pushed 64-bits at a time. We opted to have operands pushed over the two 64-bit
buses instead of having the accelerator fetch operands from memory. This primarily 
comes at the cost of register space in the rocket core (correct?), as it only takes 
four to twelve instructions to push all operands to the accelerator. In addition to 
setting the operands, the elliptic curve parameters must also be sent. This is 256 bits
for the modulus of the curve, and 256 bits for the "a" parameter of the curve, which
is needed for the point doubling operation. Once a curve is set, it will most likely
be used many times, so this extra overhead of four instructions is also 
negligible. 

One of the biggest challenges in hardware was making sure that the result of each 
operation did not exceed the modulus. This means that on each addition becomes an 
add, a comparision, and potentially a subtraction. The modular multiplication then 
also consists of repeated addition and shifting instead of a large multiply followed
by repeated subtraction of the modulus. The paths with the largest propogation delay
have full adder cells, and it is likely due to the add-check-subtract operation with 
256-bit fields, which is used numerous times throughout the functional units. This 
limits the clock period to 3.5 ns. It is not obvious how to best pipeline this due 
to the nature of the design, as there would be a tremendous cost in pipeline registsrs, 
but it should be explored in the future. 

Currently, we only have a simulated version of the rocket interface working correctly. 
We have not been able to get the compilation of our interface with the rocket to correctly
compile into Verilog code. It is unclear why the C++ backend works in this case while the Verilog
does not. 

Our hardware was written using Chisel. Verilog code was generated from Chisel and run through
the Synopsys tool flow using a 32nm educational standard cell library. Multi-threshold cells were
used, which cut the power numbers nearly in half. 

\section{Benchmarking Methodology}

FIXME: Here's where I'll introduce the software shim that we wrote,
and discuss why we're comparing with OpenSSL on x86.

\section{Results}

The numbers obtained only account for the accelerator standalone. Further work should
include the integrated design. The power numbers were drawn from Primetime Power, area 
and speed numbers from Place and Route results, and cycles from the C++ simulator, because
we did not need to access memory. 

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{c|ccc}
      Platform        & Power & Speed  & Area \\
                      & mJ/op & op/sec & mm$^2$ \\
      \hline
      OpenSSL (45nm)  & 20    & 1000   &      \\
      x86 (45 nm)     & 4000  & 5      &      \\
      Rocket          & 800   & 0.05   &      \\
      Virtex 2 (90nm) & 4000  & 250    &      \\
      Virtex 6 (45nm) & 500   & 4000   &      \\
      \hline
      Mod Mul         & 200   & 0.3    & 0.04 \\
      Mod Mul+Div     & 2     & 20     & 0.10 \\
      Point Add+Dbl   & 0.5   & 100    & 0.31 \\
      Point Mul       & 0.06  & 400    & 0.35 \\
    \end{tabular}
  \end{center}

  \caption{Comparision of results to previous work
    \label{results}}
\end{table}

FIXME: What to say here?  Maybe a graph of those area/power numbers?

\section{Future Work}

Due to time constraints, there are many useful improvements that could be made
to our accelerator. Elliptic field arthimetic is not an easy concept to grasp, and
we had to make due with using only basic algorithms. One improvement that could be made
on the ellipic field arthimetic is transforming a two-dimensional point into a three-dimensional
represenation for point multiplication. In this case, only one modular inversion is needed
for the entire point multiplication (as opposed to hundreds). This further might make it 
unnecessary for an inversion hardware unit to be needed at all. Another improvement is to use
Montgomery Multiplication\cite{mmm-hw_ecc}. This is a complex algorithm that performs successive 
multiplications fast after some intial overhead. If these algorithmic enhancements were made we
would surely see a tremendous speedup. Both of these improvements have interesting time/space tradeoffs 
for hardware implementations, so could serve as additional design points for our accelerator.

A primary concern should be getting our software
implementation up to snuff -- this almost certainly means adopting
OpenSSL as our testbench, hand-tuning its assembly for RISC-V, and
then modifying it to be easily paramaterizable so it can target our
family of accelerators. We will then have to modify our hardware 
implementation to take into account the algorithmic improvements that 
OpenSSL brings.

In addition to algorithmic improvements, there are a number of tricks
one can play to squeeze performance out of ECDSA.  The most important
one appears to be to build hand-crafted routines for operating on
particular curves -- for example, a mod routine for NIST's $p256$
curve is hundredes of times faster than a generic
one\cite{nist-routines}.

There are a number of architectural improvements that can be made to
our coprocessor interface that can help mitigate the large overhead
involved -- either placing the accelerator closer to the pipeline or
moving more state into the accelerator.  Both of these lower the
round-trip CPU to accelerator overhead and make some smaller
accelerators possible, thus providing more interesting design points.

There are also a number of interesting micro-architectural
improvements that can be made to our accelerator.  We only have a
300MHz clock, which seems exceedingly slow for what should be a
critical path of a single 256-bt adder.  Almost certainly what
happened was that there are actually two adders on the critical path,
as there are cases where one step of an algorithm requires two
additions back-to-back.  Tweaking these state machines should provide
an interesting trade-off between speed and power usage: essentially
the higher clock rate will keep more transistors toggling, they just
will not all be doing useful work.

Finally, a number of benchmarking improvements that would be
interesting to see done.  Specifically, it would be good to see a
full-system benchmark running with our accelerator attached.  This
would be particularly interesting because it would allow for some
degree of parallelism to be exposed by our accelerator to the
benchmark.  This could be very interesting because we may be able to
provide light-weight accelerator contexts (compared to POSIX threads),
which could result in an interesting userspace wrapper.

\section{Course Feedback}

CS250 was a great course for learning how to design VLSI chips using the
toolchain. It was difficult learning a new programming language to construct
hardware while also learning how to work with the VLSI toolflow. This could have 
been assisted with better tutorials on Chisel, specifically with writing test harnesses
for both the C++ simulator and more importantly the Verilog backend. 

It would also have been helpful if instead of giving us ready-made directories and skeleton
code in the early labs we were given step-by-step instructions on how to make a Chisel project
from scratch and convert it to C++ and Verliog, because we mostly ended up hacking the the labs
in really ugly ways with our accelerator to push it through the toolflow. 

\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
